{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## AI300\n",
        "### Linear Regression\n",
        "\n",
        "To calculate the maximum likelihood estimations, we first find the likelihood function. This would just be normal distribution of $y^{(n)} = N(\\theta^Tx^{(n)}, \\sigma^2)$ since $\\epsilon^{(n)}$ is normally distributied and centered at $0$. As a result, the likelihood function is the following.\n",
        "$$\n",
        "L(\\theta, \\sigma^2: y, X) = (2\\pi\\sigma^2)^{-N/2} \\exp(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{N} (y^{(n)} - \\theta^T x^{(n)})^2)\n",
        "$$\n",
        "Now, we can find the log likelihood.\n",
        "$$\n",
        "l(\\theta, \\sigma^2: y, X) = \\ln(L(\\theta, \\sigma^2: y, X)) \\\\\n",
        "l(\\theta, \\sigma^2: y, X) = \\ln(2\\pi\\sigma^2)^{-N/2} + \\ln(\\exp(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{N} (y^{(n)} - \\theta^Tx^{(n)})^2) \\\\\n",
        "l(\\theta, \\sigma^2: y, X) = -\\frac{N}{2} \\ln(2\\pi) - \\frac{N}{2} \\ln(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{N} (y^{(n)} - \\theta^Tx^{(n)})^2\n",
        "$$\n",
        "Now, we can find the maximum likelihood estimation for $\\theta$ and $\\sigma^2$.\n",
        "$$\n",
        "\\max_{\\theta, \\sigma^2} l(\\theta, \\sigma^2: y, X)\n",
        "$$\n",
        "Finding the gradient and setting it to $0$ gives the following.\n",
        "$$\n",
        "\\theta = (X^TX)^{-1}X^Ty \\\\\n",
        "\\sigma^2 = \\frac{1}{N} \\sum_{i=1}^{N} (y^{(n)} - \\theta^Tx^{(n)})^2\n",
        "$$"
      ],
      "metadata": {
        "id": "9aKFLNpqeCVz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To find $\\nabla_\\theta L(\\theta)$:\n",
        "$$\n",
        "L(\\theta) = \\frac{1}{N} \\sum^{N-1}_{n=0}(y^{(n)} - \\hat{y}^{(n)})^2 \\\\\n",
        "L(\\theta) = \\frac{1}{N} ||y - X\\theta||_2^2 = \\frac{1}{N} (y - X\\theta)^T(y-X\\theta) \\\\\n",
        "L(\\theta) = \\frac{1}{N} (y^Ty - y^TX\\theta - \\theta^TX^Ty + \\theta^TX^TX\\theta) \\\\\n",
        "L(\\theta) = \\frac{1}{N} (y^Ty - 2\\theta^TX^Ty + \\theta^TX^T\\theta) \\\\\n",
        "\\nabla_\\theta L(\\theta) = \\frac{1}{N}(-2X^Ty + 2X^TX\\theta) = \\frac{2}{N} (-X^Ty + X^TX\\theta) \\\\\n",
        "\\nabla_\\theta L(\\theta)  = \\frac{2}{N}X^T(-y + X\\theta)\n",
        "$$\n",
        "Now, we can find $\\nabla_\\theta^2 L(\\theta)$.\n",
        "$$\n",
        "\\nabla_\\theta^2 L(\\theta) = \\frac{2}{N}X^TX\n",
        "$$\n",
        "$\\frac{2}{N}X^TX$ is positive semi-definite since we're just multiplying $X^T$ by $X$, so all of the entires must be positive."
      ],
      "metadata": {
        "id": "ePAcAucMCzO0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we can find $X^TX$.\n",
        "$$\n",
        "X^TX = (U\\Sigma V^T)^T (U\\Sigma V^T) \\\\\n",
        "X^TX = V \\Sigma^T U^T U \\Sigma V^T\n",
        "$$\n",
        "We know $U$ has orthonormal columns so $U^TU = I$.\n",
        "$$\n",
        "X^TX = V\\Sigma^T \\Sigma V^T\n",
        "$$\n",
        "Here, $\\Sigma^T \\Sigma$ is a diagonal matrix of entires $\\sigma_i^2$. Now:\n",
        "$$\n",
        "V\\Sigma^T \\Sigma V^T = \\sum_{i=1}^{r-1} V \\cdot \\text{diag}(\\sigma_i^2) \\cdot V^T  \\\\\n",
        "= \\sum_{i=1}^{r-1} \\sigma_i^2 v_i v_i^T\n",
        "$$\n",
        "\n",
        "Now, we can find $\\theta$.\n",
        "$$\n",
        "X^T X = V(\\Sigma^T \\Sigma)V^T = V \\Sigma^2 V^T\n",
        "$$\n",
        "We also know the following.\n",
        "$$\n",
        "X^T y = V \\Sigma U^T y\n",
        "$$\n",
        "So we can substitute into $X^T X \\theta= X^T y$.\n",
        "$$\n",
        "(V\\Sigma^2 V^T) \\theta = V\\Sigma U^T y\n",
        "$$\n",
        "We can use pesudoinverse now. We first multiply everything by $V^T$ to cancel things out.\n",
        "$$\n",
        "V^T (V\\Sigma^2 V^T) \\theta = V^T (V\\Sigma U^T y) \\\\\n",
        "\\Sigma^2 V^T \\theta = \\Sigma U^T y\n",
        "$$\n",
        "We now multiply both sides by $\\Sigma^{-2}$ and $V$ to isolate $\\theta$.\n",
        "$$\n",
        "\\theta = V\\Sigma^{-1} U^Ty\n",
        "$$\n",
        "\n",
        "Part c: I'm actually a little confused on this one, so I'm sorry for not providing an answer. could I have a solution if there is one? Thanks :)\n",
        "\n",
        "For the first gradient, we can substitute SVD of $X^T$ and $X$. We can get SVD of $X^T$ by taking transpose of SVD of $X$ and not touching the singular values, while using transposing and switching $u$ and $v$. This gives us $\\sum_{i=0}^{r-1} \\sigma_i v_i u_i^T$\n",
        "$$\n",
        "\\nabla_\\theta L(\\theta) = \\frac{2}{N} X^T (-y + X \\theta) \\\\\n",
        "= \\frac{2}{N} X^T = \\sum_{i=0}^{r-1} (\\sigma_i v_i u_i^T)(-y + \\sum_{i=0}^{r-1} (\\sigma_i u_i v_i^T) \\theta)\n",
        "$$\n",
        "\n",
        "For the second gradient, we know the SVD of $X^T$. \\sigma_i So it might just be this.\n",
        "$$\n",
        "\\nabla_\\theta^2 L(\\theta) = \\frac{2}{N} X^TX \\\\\n",
        "= \\frac{2}{N} \\sum_{i=1}^{r-1} \\sigma_i^2 v_i v_i^T\n",
        "$$\n",
        "\n",
        "Gradient descent doesn't utilize $X^TX$. It only uses an initial estimate, a learning rate, and the gradient. We don't need to solve any systems. As a result, it's not a concern if $X^TX$ isn't invertible. \\\n",
        "\n",
        "I don't think that this statement is true. The gradient descent is:\n",
        "$$\n",
        "  \\theta^{k+1} = \\theta^{(k)} - \\alpha \\nabla f(\\theta^{(k)})\n",
        "$$\n",
        "And we know the gradient is $X^T (X\\theta-y)$. So:\n",
        "$$\n",
        "\\theta^{(k+1)} = \\theta^{(k)} - \\alpha X^T (X\\theta^{(k)}-y) \\\\\n",
        "= (I - \\alpha X^TX)\\theta^{(k)} + \\alpha X^T y\n",
        "$$\n",
        "So with a good $\\alpha$, the limit might actually be the following:\n",
        "$$\n",
        "\\lim_{k \\rightarrow \\infty} = X^+ y\n",
        "$$\n"
      ],
      "metadata": {
        "id": "BMa6JEmkxg5b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The number of columns would be the following.\n",
        "$$\n",
        "M + \\sum_{i=M}^{d_0-1} K_i\n",
        "$$\n",
        "where $M$ would represent the numerical columns and the sum would be the one-hot encoding section since we multiply the columns we need to encode by the number of classes.\n",
        "\n",
        "The maximum rank would be the following.\n",
        "$$\n",
        "\\text{rank} \\leq \\min{(N, M + \\sum_{i=M}^{d_0-1} K_i)}\n",
        "$$\n",
        "since the maximum rank is the minimum of the rows and columns (columns from expression in part A)."
      ],
      "metadata": {
        "id": "JzhpG0kJxiYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from typing import Literal\n",
        "\n",
        "class MyLinearRegression():\n",
        "  def __init__(self):\n",
        "    self._coef = None\n",
        "\n",
        "  def fit(self, X_train, y_train):\n",
        "    X_train_T = X_train.T\n",
        "    self._coef = np.linalg.inv(X_train_T @ X_train) @ X_train_T @ y_train\n",
        "\n",
        "  def predict(self, X_test):\n",
        "    prediction = X_test @ self._coef\n",
        "    return prediction\n",
        "\n",
        "  def score(self, X_test, y_test, metric: Literal[\"MAE\", \"MSE\", \"RMSE\", \"R^2\"]):\n",
        "    y_pred = self.predict(X_test)\n",
        "\n",
        "    if metric == \"MAE\":\n",
        "      return np.mean(np.abs(y_pred - y_test))\n",
        "\n",
        "    elif metric == \"MSE\":\n",
        "      return np.mean((y_pred - y_test) ** 2)\n",
        "\n",
        "    elif metric == \"RMSE\":\n",
        "      return np.sqrt(np.mean(np.abs(y_pred - y_test)))\n",
        "\n",
        "    elif metric == \"R^2\":\n",
        "      sse = np.sum((y_pred - y_test) ** 2)\n",
        "      sst = np.sum((y_test - np.mean(y_test)) ** 2)\n",
        "      return (1 - (sse / sst))\n",
        "\n",
        "    else:\n",
        "      raise ValueError(f\"argument 'metric' must be one of {\"MAE\", \"MSE\", \"RMSE\", \"R^2\"}. Received {metric}\")"
      ],
      "metadata": {
        "id": "pemp21rO2KEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyNewtonMethod():\n",
        "  def __init__(self):\n",
        "    self._coef = None\n",
        "\n",
        "  def fit(self, X_train, y_train, num_epochs, lr):\n",
        "    N = X_train.shape[0]\n",
        "    coef_ = np.zeros(X_train.shape[1])\n",
        "\n",
        "    U, S, Vt = np.linalg.svd(X_train)\n",
        "    middle_sum = sum = np.diag((1 / (S ** 2))) @ Vt.T @ Vt\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "      aN2 = (lr * N) / 2\n",
        "      grad = 2 * X_train.T @ (X_train @ coef_ - y_train)\n",
        "\n",
        "      coef_ = coef_ - (aN2 * (sum @ middle_sum))\n",
        "\n",
        "def predict(self, X_test):\n",
        "  prediction = X_test @ self._coef\n",
        "  return prediction\n",
        "\n",
        "def score(self, X_test, y_test, metric: Literal[\"MAE\", \"MSE\", \"RMSE\", \"R^2\"]):\n",
        "    y_pred = self.predict(X_test)\n",
        "\n",
        "    if metric == \"MAE\":\n",
        "      return np.mean(np.abs(y_pred - y_test))\n",
        "\n",
        "    elif metric == \"MSE\":\n",
        "      return np.mean((y_pred - y_test) ** 2)\n",
        "\n",
        "    elif metric == \"RMSE\":\n",
        "      return np.sqrt(np.mean(np.abs(y_pred - y_test)))\n",
        "\n",
        "    elif metric == \"R^2\":\n",
        "      sse = np.sum((y_pred - y_test) ** 2)\n",
        "      sst = np.sum((y_test - np.mean(y_test)) ** 2)\n",
        "      return (1 - (sse / sst))\n",
        "\n",
        "    else:\n",
        "      raise ValueError(f\"argument 'metric' must be one of {\"MAE\", \"MSE\", \"RMSE\", \"R^2\"}. Received {metric}\")"
      ],
      "metadata": {
        "id": "EaNGKMxf6re8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = pd.read_csv(\"insurance.csv\")\n",
        "\n",
        "print(\"Number of missing data:\", df.isna().sum().sum())\n",
        "print(\"Number of duplicate data:\", df.duplicated().sum(), \"\\n\")\n",
        "\n",
        "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
        "one_hot_encoder.fit(df[['sex', 'smoker', 'region']])\n",
        "encoded = one_hot_encoder.transform(df[['sex', 'smoker', 'region']])\n",
        "encoded_df = pd.DataFrame(encoded, columns=one_hot_encoder.get_feature_names_out(['sex', 'smoker', 'region']))\n",
        "\n",
        "df = df.drop(columns=['sex', 'smoker', 'region'])\n",
        "df = pd.concat([df, encoded_df], axis=1)\n",
        "\n",
        "X, y = df.drop('charges', axis=1), df['charges']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=15)\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(f\"Model Coefficients: {model.coef_}\")\n",
        "print(f\"Model intercept: {model.intercept_}\")\n",
        "y_pred = model.predict(X_test)\n",
        "print(f\"Model R^2 score: {model.score(X_test, y_test)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hzwhm9F_pgC",
        "outputId": "15b32ec4-07a4-4872-d52f-52f6f3485eb7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of missing data: 0\n",
            "Number of duplicate data: 1 \n",
            "\n",
            "Model Coefficients: [   261.96351497    339.51178314    607.21007724    -21.2664388\n",
            "     21.2664388  -11939.49230051  11939.49230051    610.61863188\n",
            "    216.02962359   -463.0899505    -363.55830497]\n",
            "Model intercept: -906.9900960683353\n",
            "Model R^2 score: 0.7709928565663493\n"
          ]
        }
      ]
    }
  ]
}